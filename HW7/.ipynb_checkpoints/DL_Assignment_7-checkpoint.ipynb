{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "*This is an assignment based on the Tensorflow provided example.*\n",
    "\n",
    "\n",
    "In this assignment, you will implement a seq2seq model with different configurations for machine translation task in NLP. you will: \n",
    "- Customize two different architectures in seq2se2.\n",
    "- Implement a simple decoder for the machine translation task using your customized layers.\n",
    "\n",
    "Try to keep your model as simple as possible (minimum number of layers and neurons with acceptable performance).\n",
    "\n",
    "**Notes:** \n",
    "- When you submit your assignment, the output of every cell should be visible.\n",
    "- You are not eligible to change any parts of the code except the predefined sections.\n",
    "- You can add your implementation only in the predefined sections.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnxXKDjq3jEL"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfodePkj3jEa"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "```\n",
    "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
    "```\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "1. Add a *start* and *end* token to each sentence.\n",
    "2. Clean the sentences by removing special characters.\n",
    "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "4. Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd0jw-eC3jEh"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opI2GzOt479E",
    "outputId": "72a9ae7c-e5b5-48c7-d4a2-414250f35003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHn4Dct23jEm"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTbSbBz55QtF",
    "outputId": "bd313c87-3521-48f8-db98-d8dd666e0b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIOn8RCNDJXG"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOi42V79Ydlr"
   },
   "source": [
    "### Limit the size of the dataset to experiment faster (optional)\n",
    "\n",
    "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with fewer data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file,\n",
    "                                                                num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QILQkOs3jFG",
    "outputId": "1943f39d-64b0-41f1-c197-56fae6de8f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJPmLZGMeD5q"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print(f'{t} ----> {lang.index_word[t]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXukARTDd7MT",
    "outputId": "12f546d6-746a-4e19-cf3e-49959a5ca11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "17 ----> se\n",
      "152 ----> les\n",
      "135 ----> ve\n",
      "2427 ----> contentos\n",
      "96 ----> hoy\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "28 ----> they\n",
      "88 ----> look\n",
      "101 ----> happy\n",
      "110 ----> today\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgCLkfv5uO3d"
   },
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "276c7a51-97f2-43cf-d2af-1c2cb715a269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNfHIF71ulLu"
   },
   "source": [
    "## Write the encoder and decoder model\n",
    "\n",
    "This is a simple encoder-decoder model for machine translation.\n",
    "\n",
    "<img src=\"https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png\" width=\"500\" alt=\"attention mechanism\">\n",
    "\n",
    "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wSMiUqd-ym8"
   },
   "source": [
    "<font color=\"red\"><b>What is the main drawback of this architecture?</b></font>\n",
    "<br>\n",
    "<font color=\"red\" size=4><div dir=rtl>گلوگاه این معماری در بهبود کارایی استفاده از یک بردار با طول ثابت است  که ترجمه را برای جملات طولانی به خصوص جملاتی که از متون آموزش طولانی‌تر هستند را مشکل می‌‌کند. هر چه قدر جمله طولانی‌تر شود کارایی انکودر - دیکودر پایه به شدت بدتر می‌شود. </div></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60gSVh05Jl6l",
    "outputId": "7c86c093-2610-4e16-8ad6-b6c566c85965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units,BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hz0MwGu2_aOO"
   },
   "source": [
    "Now, it's your turn. Implement a layer that instead of passing the last encoder output to the decoder, makes an average vector of all encoder outputs in all time step. we call it *cotext vector*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umohpBN2OM94"
   },
   "outputs": [],
   "source": [
    "class Context_vector(tf.keras.layers.Layer):\n",
    "  def __init__(self):\n",
    "    super(Context_vector, self).__init__()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    context_vector = tf.reduce_mean(inputs, axis=1)\n",
    "    # context_vector = tf.nn.softmax(context_vector)\n",
    "    return context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gHJ3ptVExO4"
   },
   "source": [
    "Implement another layer that makes a weighted average of encoder outputs as the context vector. The weights should be learned during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRlMKzFfD30J"
   },
   "outputs": [],
   "source": [
    "class Weighted_Context_vector(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    super(Weighted_Context_vector, self).__init__()\n",
    "    \n",
    "    self.Weight = tf.keras.layers.Dense(units)\n",
    "    \n",
    "\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    \n",
    "    score = self.Weight(inputs)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    weighted_context_vector = attention_weights * inputs\n",
    "    weighted_context_vector = tf.reduce_sum(weighted_context_vector, axis=1)\n",
    "\n",
    "\n",
    "    return weighted_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k534zTHiDjQU",
    "outputId": "8bfdb22c-9353-4e2c-d3ea-730703e26a7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cotext layer result shape: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "context_layer = Context_vector()\n",
    "context_result = context_layer(sample_output)\n",
    "\n",
    "print(\"Cotext layer result shape:\", context_result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mNKi3koLKeeg",
    "outputId": "e5ca6ac7-10bf-43e7-8ae5-7969e5d398d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Cotext layer result shape:  (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "weighted_context_layer = Weighted_Context_vector(sample_output.shape[-1])\n",
    "weighted_context_result = weighted_context_layer(sample_output)\n",
    "\n",
    "print(\"Weighted Cotext layer result shape: \", weighted_context_result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJLKFlTqI2fD"
   },
   "source": [
    "<font color=\"red\"><b>Explain the advantages of the (weighted) context vector you have just implemented.\n",
    "\n",
    "How could these architectures solve the vanishing gradient problem?</b></font>\n",
    "\n",
    "<font color=\"red\" size=3><div dir=rtl><b> مزیت بردارهای زمینه‌ی وزن‌دار ایجاد مکانیزم توجه است. یعنی تصمیم گرفته می‌شود چه کلماتی از جمله‌ی مبدا در ترجمه‌ی یک کلمه اهمیت بیشتری دارد و چه کلماتی هیچ اهمیتی ندارند</b> </div></font>\n",
    "<br/>\n",
    "<font color=\"red\" size=3><div dir=rtl><b>مشکل ناپدید شدن گرادیان با استفاده از لایه‌های بازگشتی GRU \n",
    "حل شده است. در این لایه از یک مکانیزم میانگین گرفتن وزن‌دار استفاده می‌شود که تصمیم می‌گیرد چه اطلاعاتی از گذشته مهم هستند و باید آنها را به خاطر سپرد. پارامترهای لایه‌ای که برای این امر استفاده می‌شود در آموزش مدل به دست می‌آید. در این لایه به نوعی تصمیم گرفته می‌شود به چه اطلاعاتی باید اهیت داد و به چه اطلاعاتی خیر، که یادآور مکانیزم توجه است.</b> </div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmTdf4MbFott"
   },
   "source": [
    "Implement **two** differen decoders using the Context vector and weighted context vector. and then try both of them with making two different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class DecoderVN(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(DecoderVN, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    self.context_layer = Context_vector()\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    context_vector = self.context_layer(enc_output)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "   \n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCUMzjoXGy0U"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    self.weighted_context_layer = Weighted_Context_vector(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    ############################################\n",
    "    ########put your implementation here########\n",
    "    ############################################\n",
    "    weighted_context_vector = self.weighted_context_layer(enc_output)\n",
    "    x = tf.concat([tf.expand_dims(weighted_context_vector, 1), x], axis=-1)\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEuEpdpJgKY8",
    "outputId": "f1c41c1d-1838-45be-f5c4-a25c82ba2b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape:  (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _= decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: ', sample_decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch_71VbIRfK"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMVWzzsfNl4e"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpObfY22IddU"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdCLdgGvHkax"
   },
   "source": [
    "\n",
    "\n",
    "<font color=\"red\"><b>Explain *teacher forcing technique* and tell us why it is necessary for training seq2seq models?\n",
    "\n",
    "(It is used in training step function below)</b></font>\n",
    "\n",
    "<font color=\"red\" size=3><div dir=rtl><b>  \n",
    "کمک معلم یا اصطلاحا Teacher Forcing روش سریع و کارآمدی است که برای آموزش مدلهای مبتنی بر شبکه های عصبی بازگشتی که از خروجی  یک گام قبل بعنوان ورودی بهره میبرند مورد استفاده قرار میگیرد. این متد یک روش آموزش شبکه است که در توسعه مدلهای زبانی مبتنی بر یادگیری عمیق که در حوزه های گوناگون من جمله ترجمه ماشینی،  خلاصه سازی متن و شرح نویسی تصویر مورد استفاده قرار میگیرند نقش حیاتی دارد.\n",
    "<br/> استفاده از خروجی بعنوان ورودی در پیش بینی دنباله : مدلهای پیش بینی دنباله ای وجود دارند که در آنها از خروجی تولید شده در اخرین گام زمانی o_{t-1} بعنوان ورودی برای مدل در گام زمانی فعلی (X_t)بهره برده میشود. این گونه مدلها در مدلهای زبانی که خروجی هر گام، یک کلمه بوده و سپس این خروجی بعنوان ورودی گام زمانی بعدی مورد استفاده قرار میگیرد تا کلمه بعدی در دنباله ایجاد شود رایج است. بعنوان مثال این گونه مدل های زبانی در معماری های شبکه عصبی بازگشتی Encoder-Decoder برای مسائل تولید دنباله به دنباله (Sequence to sequence) ای همانند : \n",
    "<br/>ترجمه ماشینی (Machine Translation) \n",
    "<br/>تولید عنوان (Caption Generation) \n",
    "<br/>خلاصه سازی متن (Text Summarization) \n",
    "<br/>مورد استفاده قرار میگیرند. بعد از آنکه مدل اموزش دید. میتوان از توکن (نشانه) “شروع دنباله” برای آغاز فرآیند استفاده کرد و یک کلمه را درگام زمانی اول تولید کرد حالا از این خروجی (کلمه تازه) بعنوان ورودی برای گام زمانی دوم استفاده میشود و خروجی آن بعنوان ورودی برای گام زمانی بعدی و همینطور الی اخر استفاده میشود.\n",
    "<br/>\n",
    "در مدل‌های Sequence to sequence\n",
    "اگر از روش‌ teaching force استفاده نکنیم مدل دنباله‌های بی‌ربط زیادی را برای پیشبینی دنباله‌ی بعدی ارزیابی می‌کند که باعث می‌شود فرایند یادگیری کند و مدل ناپایدار شود.\n",
    " </b> </div></font>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "      \n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddefjBMa3jF0",
    "outputId": "2c4f7d0b-7972-407b-f4d8-9f7ea7893a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6627\n",
      "Epoch 1 Batch 100 Loss 2.2129\n",
      "Epoch 1 Batch 200 Loss 1.9349\n",
      "Epoch 1 Batch 300 Loss 1.7198\n",
      "Epoch 1 Loss 2.0042\n",
      "Time taken for 1 epoch 41.40 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4887\n",
      "Epoch 2 Batch 100 Loss 1.4764\n",
      "Epoch 2 Batch 200 Loss 1.3450\n",
      "Epoch 2 Batch 300 Loss 1.2543\n",
      "Epoch 2 Loss 1.3662\n",
      "Time taken for 1 epoch 31.21 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.0249\n",
      "Epoch 3 Batch 100 Loss 1.0560\n",
      "Epoch 3 Batch 200 Loss 1.0502\n",
      "Epoch 3 Batch 300 Loss 0.9783\n",
      "Epoch 3 Loss 0.9680\n",
      "Time taken for 1 epoch 31.09 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6839\n",
      "Epoch 4 Batch 100 Loss 0.7243\n",
      "Epoch 4 Batch 200 Loss 0.6856\n",
      "Epoch 4 Batch 300 Loss 0.6731\n",
      "Epoch 4 Loss 0.6509\n",
      "Time taken for 1 epoch 31.52 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3456\n",
      "Epoch 5 Batch 100 Loss 0.4034\n",
      "Epoch 5 Batch 200 Loss 0.4316\n",
      "Epoch 5 Batch 300 Loss 0.3408\n",
      "Epoch 5 Loss 0.4213\n",
      "Time taken for 1 epoch 31.18 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2902\n",
      "Epoch 6 Batch 100 Loss 0.2711\n",
      "Epoch 6 Batch 200 Loss 0.2701\n",
      "Epoch 6 Batch 300 Loss 0.2532\n",
      "Epoch 6 Loss 0.2684\n",
      "Time taken for 1 epoch 31.71 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1081\n",
      "Epoch 7 Batch 100 Loss 0.1568\n",
      "Epoch 7 Batch 200 Loss 0.1685\n",
      "Epoch 7 Batch 300 Loss 0.1965\n",
      "Epoch 7 Loss 0.1783\n",
      "Time taken for 1 epoch 31.31 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0907\n",
      "Epoch 8 Batch 100 Loss 0.1684\n",
      "Epoch 8 Batch 200 Loss 0.1109\n",
      "Epoch 8 Batch 300 Loss 0.1763\n",
      "Epoch 8 Loss 0.1252\n",
      "Time taken for 1 epoch 31.78 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0860\n",
      "Epoch 9 Batch 100 Loss 0.0599\n",
      "Epoch 9 Batch 200 Loss 0.1013\n",
      "Epoch 9 Batch 300 Loss 0.1389\n",
      "Epoch 9 Loss 0.0954\n",
      "Time taken for 1 epoch 31.40 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0545\n",
      "Epoch 10 Batch 100 Loss 0.0547\n",
      "Epoch 10 Batch 200 Loss 0.0998\n",
      "Epoch 10 Batch 300 Loss 0.0810\n",
      "Epoch 10 Loss 0.0784\n",
      "Time taken for 1 epoch 31.88 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    \n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU3Ce8M6I3rz"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  \n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input:', sentence)\n",
    "  print('Predicted translation:', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "44bd0c1c-29cc-4eb9-8af0-76e94f7be353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8b5df7abd0>"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrAM0FDomq3E",
    "outputId": "57aad313-dd59-463a-a33c-4a1260c0d035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here is very cold here is very \n"
     ]
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSx2iM36EZQZ",
    "outputId": "c2296b53-8e29-4326-b782-f1267f7dca98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOYOAATypAQn",
    "outputId": "749df126-fc3b-4a99-ae30-0ff35a51f435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hasta la vista bebe . <end>\n",
      "Predicted translation: goodbye ! <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'Hasta La Vista Bebé.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHnKUD66iEqu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ks2OQTWmo20E",
    "outputId": "cd1960d8-e94b-4be6-a8cd-cf1073481554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape:  (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder_nv = DecoderVN(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_nv_output, _= decoder_nv(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: ', sample_decoder_nv_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYcLno_Os0BX"
   },
   "source": [
    "## Define the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZMhR2T4s0Bm"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAnXONO0s0Bo"
   },
   "source": [
    "## Checkpoints (Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC7qGNl-s0Bp"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder_nv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdrGQUT3s0Bq"
   },
   "source": [
    "## Training\n",
    "\n",
    "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
    "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFmdViyEs0Bq"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden = decoder_nv(dec_input, dec_hidden, enc_output)\n",
    "      \n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder_nv.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_rbqXsqs0Br",
    "outputId": "014e8004-a419-4e4c-c15a-eb5748fea8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.6557\n",
      "Epoch 1 Batch 100 Loss 1.5415\n",
      "Epoch 1 Batch 200 Loss 1.1169\n",
      "Epoch 1 Batch 300 Loss 1.0193\n",
      "Epoch 1 Loss 1.3630\n",
      "Time taken for 1 epoch 34.55 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6237\n",
      "Epoch 2 Batch 100 Loss 0.5785\n",
      "Epoch 2 Batch 200 Loss 0.4892\n",
      "Epoch 2 Batch 300 Loss 0.5735\n",
      "Epoch 2 Loss 0.5731\n",
      "Time taken for 1 epoch 24.92 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2661\n",
      "Epoch 3 Batch 100 Loss 0.2904\n",
      "Epoch 3 Batch 200 Loss 0.2756\n",
      "Epoch 3 Batch 300 Loss 0.3038\n",
      "Epoch 3 Loss 0.3222\n",
      "Time taken for 1 epoch 24.53 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1762\n",
      "Epoch 4 Batch 100 Loss 0.1928\n",
      "Epoch 4 Batch 200 Loss 0.2192\n",
      "Epoch 4 Batch 300 Loss 0.1763\n",
      "Epoch 4 Loss 0.2072\n",
      "Time taken for 1 epoch 24.97 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1172\n",
      "Epoch 5 Batch 100 Loss 0.1844\n",
      "Epoch 5 Batch 200 Loss 0.1662\n",
      "Epoch 5 Batch 300 Loss 0.1440\n",
      "Epoch 5 Loss 0.1520\n",
      "Time taken for 1 epoch 24.54 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0984\n",
      "Epoch 6 Batch 100 Loss 0.0842\n",
      "Epoch 6 Batch 200 Loss 0.1217\n",
      "Epoch 6 Batch 300 Loss 0.1385\n",
      "Epoch 6 Loss 0.1224\n",
      "Time taken for 1 epoch 24.43 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0676\n",
      "Epoch 7 Batch 100 Loss 0.1069\n",
      "Epoch 7 Batch 200 Loss 0.1162\n",
      "Epoch 7 Batch 300 Loss 0.1339\n",
      "Epoch 7 Loss 0.1005\n",
      "Time taken for 1 epoch 24.11 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0690\n",
      "Epoch 8 Batch 100 Loss 0.1208\n",
      "Epoch 8 Batch 200 Loss 0.0694\n",
      "Epoch 8 Batch 300 Loss 0.0690\n",
      "Epoch 8 Loss 0.0864\n",
      "Time taken for 1 epoch 24.52 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0548\n",
      "Epoch 9 Batch 100 Loss 0.0942\n",
      "Epoch 9 Batch 200 Loss 0.0952\n",
      "Epoch 9 Batch 300 Loss 0.0640\n",
      "Epoch 9 Loss 0.0766\n",
      "Time taken for 1 epoch 23.92 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0879\n",
      "Epoch 10 Batch 100 Loss 0.1121\n",
      "Epoch 10 Batch 200 Loss 0.0713\n",
      "Epoch 10 Batch 300 Loss 0.0974\n",
      "Epoch 10 Loss 0.0724\n",
      "Time taken for 1 epoch 24.45 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    \n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "  print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
    "  print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eC2cu7YCs0Bu"
   },
   "source": [
    "## Translate\n",
    "\n",
    "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "* Stop predicting when the model predicts the *end token*.\n",
    "\n",
    "\n",
    "Note: The encoder output is calculated only once for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYIgNVjBs0Bv"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  \n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden = decoder_nv(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ruY50nfs0Bw"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence = evaluate(sentence)\n",
    "\n",
    "  print('Input:', sentence)\n",
    "  print('Predicted translation:', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FugB-dVGs0Bx"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7MHBgtLs0By",
    "outputId": "c60b809d-9cb1-4f99-d661-d0bbe82a7aea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8b5d58da10>"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NB7fzZWTs0Bz",
    "outputId": "37fffe15-3372-4271-8b3f-e409e41017d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WktMzQhLs0B0",
    "outputId": "0b3ae8ba-071b-4a56-cc7c-3a7f07aa88c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebfad3Ys5rEc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Assignment_7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
